# app/network.py
from pathlib import Path
import re
import html
import logging
from typing import Optional, Dict, Union
from functools import lru_cache

import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

from .exceptions import IpBlockedError
from .archive_processor import ArchiveProcessor

try:
    from bs4 import BeautifulSoup
except ImportError:
    BeautifulSoup = None

logger = logging.getLogger(__name__)

class EHentaiHashSearcher:
    def __init__(self, cookies: Optional[Dict] = None):
        # 1. åˆå§‹åŒ–ç½‘ç»œä¼šè¯
        self.session = requests.Session()
        self._setup_session(cookies)
        
        # æ ¹æ® Cookie åˆ¤æ–­æ˜¯è¡¨ç«™è¿˜æ˜¯é‡Œç«™
        self.domain = "https://exhentai.org" if cookies and cookies.get('igneous') != 'mystery' else "https://e-hentai.org"
        self.api_url = "https://api.e-hentai.org/api.php"
        
        # 2. åˆå§‹åŒ–æœ¬åœ°å½’æ¡£å¤„ç†å™¨
        self.processor = ArchiveProcessor()
        
        # 3. [ä¼˜åŒ–] ç®€å•çš„å†…å­˜ç¼“å­˜ï¼Œé¿å…é‡å¤è¯·æ±‚ç›¸åŒçš„ç”»å»Šå…ƒæ•°æ®
        self._metadata_cache = {}

    def _setup_session(self, cookies):
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36',
        })
        # å¢åŠ é‡è¯•ç­–ç•¥
        retries = Retry(total=3, backoff_factor=1, status_forcelist=[500, 502, 503, 504])
        self.session.mount('https://', HTTPAdapter(max_retries=retries))
        
        if cookies:
            self.session.cookies.update(cookies)
        
        # å¼ºåˆ¶è®¾ç½® nw=1 (è·³è¿‡æˆäººè­¦å‘Š)
        self.session.cookies.set('nw', '1', domain='.e-hentai.org')
        self.session.cookies.set('nw', '1', domain='.exhentai.org')

    def process_archive(self, archive_path: Union[str, object], target: str = 'cover') -> Union[str, None]:
        """
        å¤„ç†å½’æ¡£æ–‡ä»¶ï¼šè®¡ç®— Hash æˆ– æå–æ ‡é¢˜ -> æœç´¢
        """
        archive_path = Path(archive_path)

        # === çº¯æ ‡é¢˜æœç´¢æ¨¡å¼ ===
        if target == 'title':
            from .utils import parse_gallery_title
            
            # è§£ææ–‡ä»¶åè·å–æ ¸å¿ƒæ ‡é¢˜
            parsed_info = parse_gallery_title(archive_path.stem)
            keyword = parsed_info.get('title')
            
            # å…œåº•ï¼šå¦‚æœè§£æç»“æœå¤ªçŸ­ï¼Œä½¿ç”¨æ–‡ä»¶å
            if not keyword or len(keyword) < 2:
                keyword = archive_path.stem
            
            logger.debug(f"ğŸ” [Scanner] æ ‡é¢˜æ¨¡å¼å¤„ç†: {keyword}")
            return self.search_by_keyword(keyword)

        # === Hash æœç´¢æ¨¡å¼ ===
        f_hash, status = self.processor.get_file_hash(archive_path, target_mode=target)
        
        if status != "OK":
            return status

        return self.search_by_hash(f_hash, is_cover=(target == 'cover'))

    def search_by_hash(self, file_hash: str, is_cover: bool = True) -> Union[str, None]:
        if not file_hash: return None
        
        params = f"f_shash={file_hash}&fs_similar=1" + ("&fs_covers=1" if is_cover else "")
        search_url = f"{self.domain}/?{params}"
        
        logger.debug(f"ğŸ” [Network] Hashæœç´¢: {file_hash[:8]}... | Mode: {'Cover' if is_cover else 'Page'}")

        try:
            response = self.session.get(search_url, timeout=30)
            
            if "Your IP address has been" in response.text:
                raise IpBlockedError("IP è¢« E-Hentai å°ç¦")

            result_url = self._parse_search_result(response.text)
            if result_url:
                logger.debug(f"âœ… [Network] æ‰¾åˆ°åŒ¹é…: {result_url}")
                return result_url
            else:
                logger.debug(f"âšª [Network] æœªæ‰¾åˆ°åŒ¹é… (No Match)")
                return "NO_MATCH"
        except requests.exceptions.RequestException as e:
            logger.warning(f"âš ï¸ [Network] è¯·æ±‚å¤±è´¥: {e}")
            return None

    def search_by_keyword(self, keyword: str) -> Union[str, None]:
        if not keyword: return None
        
        logger.debug(f"ğŸ” [Network] æ–‡æœ¬æœç´¢: {keyword}")
        params = {"f_search": keyword, "f_apply": "Apply Filter"}

        try:
            response = self.session.get(self.domain + "/", params=params, timeout=30)
            if "Your IP address has been" in response.text:
                raise IpBlockedError("IP è¢« E-Hentai å°ç¦")

            result_url = self._parse_search_result(response.text)
            if result_url:
                logger.info(f"âœ… [Network] æ–‡æœ¬åŒ¹é…æˆåŠŸ: {result_url}")
                return result_url
            else:
                logger.debug(f"âšª [Network] æ–‡æœ¬æœªæ‰¾åˆ°åŒ¹é…")
                return "NO_MATCH"
        except requests.exceptions.RequestException as e:
            logger.warning(f"âš ï¸ [Network] æœç´¢è¯·æ±‚å¤±è´¥: {e}")
            return None

    def get_gallery_metadata(self, gallery_url: str) -> Optional[Dict]:
        """æ ¹æ® URL è·å–å…ƒæ•°æ® (å¸¦ç¼“å­˜)"""
        match = re.search(r'/g/(\d+)/([\w]+)', gallery_url)
        if not match: return None
        
        gid, token = int(match.group(1)), match.group(2)
        cache_key = f"{gid}_{token}"

        # [ä¼˜åŒ–] æ£€æŸ¥ç¼“å­˜
        if cache_key in self._metadata_cache:
            logger.debug(f"âš¡ [Cache] å‘½ä¸­å…ƒæ•°æ®ç¼“å­˜: {gid}")
            return self._metadata_cache[cache_key]

        logger.debug(f"â˜ï¸ [API] è·å–å…ƒæ•°æ®: GID={gid}")

        payload = {
            "method": "gdata",
            "gidlist": [[gid, token]],
            "namespace": 1
        }

        try:
            res = self.session.post(self.api_url, json=payload, timeout=30)
            res.raise_for_status()
            data = res.json()
            
            if not data.get('gmetadata'): 
                logger.warning(f"âš ï¸ [API] æœªè¿”å› gmetadata æ•°æ®")
                return None
            
            gmeta = data['gmetadata'][0]
            
            title_jpn = html.unescape(gmeta.get('title_jpn') or "")
            title_en = html.unescape(gmeta.get('title') or "")
            final_title = title_jpn if title_jpn else title_en
            
            tags = gmeta.get('tags', [])
            if category := gmeta.get('category'):
                tags.append(f"reclass:{category.lower()}")
            
            result = {
                "title": final_title,
                "title_jpn": title_jpn,
                "title_en": title_en,
                "tags": tags,
                "uploader": gmeta.get('uploader'),
                "category": category
            }
            
            # [ä¼˜åŒ–] å†™å…¥ç¼“å­˜
            self._metadata_cache[cache_key] = result
            return result

        except Exception as e:
            logger.warning(f"âš ï¸ [API] è·å–å…ƒæ•°æ®å¼‚å¸¸: {e}")
            return None

    def _parse_search_result(self, html_content: str) -> Optional[str]:
        """è§£ææœç´¢ç»“æœé¡µé¢"""
        if "/g/" not in html_content:
            return None

        # ä¼˜å…ˆä½¿ç”¨ BeautifulSoup è§£æï¼Œæ›´å‡†ç¡®
        if BeautifulSoup:
            try:
                soup = BeautifulSoup(html_content, 'html.parser')
                # æŸ¥æ‰¾ class="gl3c glname" çš„ div (PCç«¯) æˆ–è€…ç›´æ¥æŸ¥æ‰¾é“¾æ¥
                for a_tag in soup.find_all('a', href=True):
                    href = a_tag['href']
                    # åŒ¹é… /g/12345/abcdef/ æ ¼å¼
                    if re.search(r'/g/\d+/[a-z0-9]+', href):
                        if self.domain in href:
                            return href
                        elif href.startswith("/"):
                            return self.domain + href
            except Exception:
                pass

        # æ­£åˆ™å…œåº•
        match = re.search(r'https?://e[x-]?hentai\.org/g/\d+/[a-z0-9]+/', html_content)
        if match: return match.group(0)
        
        return None