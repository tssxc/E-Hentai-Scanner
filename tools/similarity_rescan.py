# tools/similarity_rescan.py
import os
import sys
import logging
import re
import html
import time
import traceback

# 1. è·¯å¾„è®¾ç½®ï¼šç¡®ä¿èƒ½æ‰¾åˆ°é¡¹ç›®æ ¹ç›®å½•å’Œ app æ¨¡å—
# è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½• (tools/)
current_dir = os.path.dirname(os.path.abspath(__file__))
# è·å–é¡¹ç›®æ ¹ç›®å½• (E-Hentai-Scanner/)
project_root = os.path.dirname(current_dir)

# å°†é¡¹ç›®æ ¹ç›®å½•åŠ å…¥ Python æœç´¢è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥ app.xxx
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# 2. å¯¼å…¥é¡¹ç›®æ¨¡å— (æ­¤æ—¶å·²åœ¨ path ä¸­)
try:
    from app import config
    from app.common import initialize_components
    from app.utils import calculate_similarity, perform_random_sleep, parse_gallery_title
    from app.scanner_core import scan_single_file
    from app.logger import setup_logging
except ImportError as e:
    print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
    print("   è¯·ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹è¿è¡Œï¼Œæˆ–æ£€æŸ¥ç›®å½•ç»“æ„ã€‚")
    sys.exit(1)

# è®¾ç½®å•ç‹¬çš„æ—¥å¿—æ–‡ä»¶
LOG_FILE = os.path.join(project_root, "logs", "similarity_rescan.log")
logger = setup_logging(LOG_FILE)

# ç›¸ä¼¼åº¦é˜ˆå€¼
SIMILARITY_THRESHOLD = 0.4

class SimilarityRescanner:
    def __init__(self):
        """
        åˆå§‹åŒ–æ‰«æå™¨ç»„ä»¶ (æ­£å¼è¿è¡Œæ¨¡å¼)
        """
        self.db = None
        self.searcher = None
        self.translator = None
        self.handler = None
        
        try:
            # è§£åŒ…ç»„ä»¶ (æ³¨æ„ï¼šæ ¹æ® app/common.py çš„å®é™…è¿”å›è°ƒæ•´è§£åŒ…æ•°é‡)
            # å‡è®¾ initialize_components è¿”å›: db, searcher, translator, task_manager, result_handler, _, _
            comps = initialize_components()
            self.db = comps[0]
            self.searcher = comps[1]
            self.translator = comps[2]
            self.handler = comps[4]
            
            logger.info("âœ… ç»„ä»¶åˆå§‹åŒ–å®Œæˆ (Toolsæ¨¡å¼)")
        except Exception as e:
            logger.error(f"âŒ ç»„ä»¶åˆå§‹åŒ–å¤±è´¥: {e}")
            logger.debug(traceback.format_exc())
            sys.exit(1)

    def close(self):
        if self.db:
            self.db.close()

    def get_detailed_metadata(self, gallery_url):
        """
        è·å–ç”»å»Šçš„è¯¦ç»†å…ƒæ•°æ®
        """
        if not gallery_url or not self.searcher:
            return None, None, []

        match = re.search(r'/g/(\d+)/([\w]+)', gallery_url)
        if not match:
            return None, None, []
        
        gid, token = int(match.group(1)), match.group(2)
        payload = {"method": "gdata", "gidlist": [[gid, token]], "namespace": 1}

        try:
            res = self.searcher.session.post(self.searcher.api_url, json=payload, timeout=10)
            data = res.json()
            
            if 'gmetadata' in data and data['gmetadata']:
                gmeta = data['gmetadata'][0]
                t_jpn = html.unescape(gmeta.get('title_jpn') or "")
                t_en = html.unescape(gmeta.get('title') or "")
                tags = gmeta.get('tags', []) 
                
                # ç¡®ä¿ä¸¤ä¸ªæ ‡é¢˜å­—æ®µéƒ½æœ‰å€¼
                if not t_jpn and t_en: t_jpn = t_en
                if not t_en and t_jpn: t_en = t_jpn
                
                return t_jpn, t_en, tags
        except Exception as e:
            logger.warning(f"âš ï¸ [Meta-Fetch] è·å–å…ƒæ•°æ®å¤±è´¥: {e}")
        
        return None, None, []

    def check_title_match(self, clean_name, title_to_check):
        """
        å•ä¸€æ ‡é¢˜ç›¸ä¼¼åº¦æ£€æµ‹
        """
        if not title_to_check:
            return False, 0.0

        # 1. ç›´æ¥ç›¸ä¼¼åº¦
        sim_direct = calculate_similarity(clean_name, title_to_check)
        
        # 2. è§£æåç›¸ä¼¼åº¦ (æå–æ ¸å¿ƒæ ‡é¢˜)
        parsed_title = parse_gallery_title(title_to_check)['title']
        sim_parsed = calculate_similarity(clean_name, parsed_title)
        
        best_score = max(sim_direct, sim_parsed)
        return best_score >= SIMILARITY_THRESHOLD, best_score

    def check_tags_coverage(self, clean_name, tags_list):
        """
        æ£€æŸ¥ Tag è¦†ç›–æƒ…å†µ
        """
        if not tags_list:
            return False

        info = parse_gallery_title(clean_name)
        target_fields = {
            'artist': info.get('artist'), 
            'group': info.get('group') # è¿™é‡Œå¯¹åº” utils.py ä¿®æ”¹åçš„ group
        }
        
        normalized_tags = [str(t).lower() for t in tags_list if t]

        def check_field_in_tags(field_value):
            if not field_value: return False
            val = field_value.lower().strip()
            if len(val) < 2: return False
            for tag in normalized_tags:
                # ç§»é™¤ namespace (å¦‚ artist:xxx -> xxx)
                tag_val = tag.split(':', 1)[1] if ':' in tag else tag
                if val in tag_val.strip(): return True
            return False

        match_log = []
        if check_field_in_tags(target_fields['artist']): match_log.append(f"Artist[{target_fields['artist']}]")
        if check_field_in_tags(target_fields['group']): match_log.append(f"Group[{target_fields['group']}]")

        if match_log:
            logger.info(f"   ğŸ·ï¸ [TagéªŒè¯æˆåŠŸ] {', '.join(match_log)}")
            return True
        return False

    def evaluate_scan_result(self, clean_name, scan_url):
        """
        æ‰§è¡Œ 4 æ­¥éªŒè¯æµç¨‹
        è¿”å›: (æ˜¯å¦æˆåŠŸ, æ ‡é¢˜, æ ‡ç­¾å­—ç¬¦ä¸²)
        """
        # 0. è·å–å…ƒæ•°æ®
        t_jp, t_en, raw_tags = self.get_detailed_metadata(scan_url)
        
        # å‡†å¤‡ç¿»è¯‘
        trans_tags = self.translator.translate_tags(raw_tags) if raw_tags else []
        combined_tags = (raw_tags or []) + trans_tags
        final_tags_str = ", ".join(combined_tags)
        final_title = t_jp or t_en # ä¼˜å…ˆå­˜æ—¥æ–‡æ ‡é¢˜

        matched = False
        log_prefix = ""

        # === Step 1: è‹±æ–‡æ ‡é¢˜æ£€æµ‹ ===
        is_match, score = self.check_title_match(clean_name, t_en)
        if is_match:
            matched = True
            log_prefix = f"âœ… [è‹±æ–‡æ ‡é¢˜åŒ¹é…] Sim:{score:.2f}"
        
        # === Step 2: åŸå§‹ Tag æ£€æµ‹ ===
        if not matched:
            if self.check_tags_coverage(clean_name, raw_tags):
                matched = True
                log_prefix = "âœ… [Raw TagåŒ¹é…]"

        # === Step 3: æ—¥æ–‡/åŸæ ‡é¢˜æ£€æµ‹ ===
        if not matched:
            is_match, score = self.check_title_match(clean_name, t_jp)
            if is_match:
                matched = True
                log_prefix = f"âœ… [æ—¥æ–‡æ ‡é¢˜åŒ¹é…] Sim:{score:.2f}"

        # === Step 4: ç¿»è¯‘ Tag æ£€æµ‹ ===
        if not matched:
            if self.check_tags_coverage(clean_name, combined_tags):
                matched = True
                log_prefix = "âœ… [Trans TagåŒ¹é…]"

        if matched:
            logger.info(f"   {log_prefix}")
            return True, final_title, final_tags_str
        
        return False, final_title, final_tags_str

    def process_single_file(self, file_path):
        """
        æ ¸å¿ƒå¤„ç†æµç¨‹
        """
        if not os.path.exists(file_path):
            return

        record = self.db.get_record_by_path(file_path)
        if not record:
            return

        file_name = os.path.basename(file_path)
        clean_name = os.path.splitext(file_name)[0]
        
        current_data = {
            'url': record['gallery_url'],
            'title': record['title'],
            'tags': record['tags']
        }

        logger.info(f"ğŸ” [å¤„ç†] {file_name}")

        # === é˜¶æ®µ 1: æ£€æŸ¥æœ¬åœ°æ•°æ®åº“ ===
        db_tags_list = [t.strip() for t in (current_data['tags'] or "").split(',')] if current_data['tags'] else []
        
        m_title, s_title = self.check_title_match(clean_name, current_data['title'])
        m_tag = self.check_tags_coverage(clean_name, db_tags_list)
        
        if m_title or m_tag:
            logger.info(f"   âœ… [æœ¬åœ°è®°å½•æœ‰æ•ˆ] TitleSim:{s_title:.2f} / TagMatch:{m_tag}")
            return

        logger.warning(f"   âš ï¸ [æœ¬åœ°æ ¡éªŒå¤±è´¥] å¼€å§‹é‡æ‰«æµç¨‹...")

        # === é˜¶æ®µ 2: å°é¢æ‰«æ ===
        resolved = False
        perform_random_sleep()
        logger.info("   ğŸ”„ å°è¯•: å°é¢æ‰«æ...")
        
        res_cover = scan_single_file(file_path, self.searcher, self.handler, scan_mode='cover')
        if res_cover['success']:
            scan_url = res_cover['url']
            is_success, new_title, new_tags = self.evaluate_scan_result(clean_name, scan_url)
            
            current_data.update({'url': scan_url, 'title': new_title, 'tags': new_tags})

            if is_success:
                resolved = True
                logger.info("   ğŸ‰ å°é¢æ‰«ææˆåŠŸå¹¶åŒ¹é…!")
                self.db.save_record(file_path, 'SUCCESS', scan_url, new_title, new_tags)

        # === é˜¶æ®µ 3: ç¬¬10é¡µæ‰«æ ===
        if not resolved:
            perform_random_sleep()
            logger.info("   ğŸ”„ å°è¯•: ç¬¬10é¡µæ‰«æ...")
            
            res_sec = scan_single_file(file_path, self.searcher, self.handler, scan_mode='second')
            if res_sec['success']:
                scan_url = res_sec['url']
                is_success, new_title, new_tags = self.evaluate_scan_result(clean_name, scan_url)
                
                current_data.update({'url': scan_url, 'title': new_title, 'tags': new_tags})

                if is_success:
                    resolved = True
                    logger.info("   ğŸ‰ ç¬¬10é¡µæ‰«ææˆåŠŸå¹¶åŒ¹é…!")
                    self.db.save_record(file_path, 'SUCCESS', scan_url, new_title, new_tags)

        # === é˜¶æ®µ 4: æœ€ç»ˆåˆ¤å®š ===
        if not resolved:
            logger.warning(f"   ğŸ“‰ [å¤±è´¥] æ‰€æœ‰æ‰‹æ®µå‡æœªåŒ¹é…ï¼Œæ›´æ–°ä¸º MISMATCH")
            self.db.save_record(
                file_path, 
                status='MISMATCH', 
                url=current_data['url'], 
                title=current_data['title'], 
                tags=current_data['tags']
            )

    def run_batch_scan(self):
        """æ‰¹é‡è¿è¡Œ"""
        all_paths = self.db.get_all_processed_paths()
        logger.info(f"ğŸ“‚ æ•°æ®åº“è®°å½•æ€»æ•°: {len(all_paths)}")
        
        count = 0
        for idx, file_path in enumerate(all_paths, 1):
            if not os.path.exists(file_path): continue
            
            # ä»…å¤„ç† SUCCESS çŠ¶æ€çš„è®°å½•
            record = self.db.get_record_by_path(file_path)
            if not record or record['status'] != 'MISMATCH': continue
            
            self.process_single_file(file_path)
            count += 1
            
            if count % 10 == 0:
                logger.info(f"â³ å·²å¤„ç† {count} ä¸ªæ–‡ä»¶...")
        
        logger.info(f"ğŸ æ‰«æä»»åŠ¡ç»“æŸï¼Œå…±å¤„ç†: {count} ä¸ª")


if __name__ == "__main__":
    app = SimilarityRescanner()
    try:
        app.run_batch_scan()
    except KeyboardInterrupt:
        print("\nğŸ›‘ ç”¨æˆ·ä¸­æ–­")
    finally:
        app.close()