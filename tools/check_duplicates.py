# tools/check_duplicates.py
import os
import sys
import shutil
import logging
import re
import difflib
from collections import defaultdict, deque
from datetime import datetime

# ================= ç¯å¢ƒè®¾ç½® =================
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from app import config
from app.database import DatabaseManager

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(message)s')
logger = logging.getLogger(__name__)

# ================= è·¯å¾„é…ç½® =================

# æ³¨æ„ï¼šè¯·ç¡®ä¿è¿™é‡Œçš„è·¯å¾„ä¸æ‚¨çš„å®é™…æ¼«ç”»è·¯å¾„ä¸€è‡´ï¼Œæˆ–è€…ä¿®æ”¹ä¸ºä» config è¯»å–
BASE_DEBUG_DIR = r"D:\æ¼«ç”»"
DUPLICATES_ROOT = os.path.join(BASE_DEBUG_DIR, "duplicates")

# ================= è¾…åŠ©å‡½æ•° =================

def ensure_duplicates_root():
    if not os.path.exists(DUPLICATES_ROOT):
        print(f"ğŸ“ åˆ›å»ºç›®å½•: {DUPLICATES_ROOT}")
        os.makedirs(DUPLICATES_ROOT)
    else:
        print(f"ğŸ“‚ é‡å¤æ–‡ä»¶åº“: {DUPLICATES_ROOT}")

def sanitize_filename(name):
    """
    [å…³é”®ä¿®å¤] æ¸…ç†æ–‡ä»¶å
    1. ç§»é™¤éæ³•å­—ç¬¦
    2. ç§»é™¤ Windows ä¸å…è®¸çš„æœ«å°¾ç‚¹(.)å’Œç©ºæ ¼
    """
    if not name: return "Unknown_Title"
    
    # ç§»é™¤æ‹¬å·å†…å®¹
    name = re.sub(r'[\[\(\{].*?[\]\)\}]', '', name) 
    
    # å°†éæ³•å­—ç¬¦æ›¿æ¢ä¸ºç©ºæ ¼
    clean = re.sub(r'[\\/*?:"<>|]', ' ', name)
    
    # [å…³é”®ä¿®å¤]: .strip(" .") ä¼šåŒæ—¶ç§»é™¤å¼€å¤´å’Œç»“å°¾çš„ ç©ºæ ¼ å’Œ ç‚¹
    # Windows æ–‡ä»¶å¤¹ä¸¥ç¦ä»¥ç‚¹ç»“å°¾
    clean = clean.strip(" .")
    
    # åˆå¹¶å¤šä½™ç©ºæ ¼
    clean = ' '.join(clean.split())
    
    if not clean:
        return "Unknown_Title"
        
    return clean[:100]

def clean_title_for_comparison(title):
    if not title: return ""
    t = title.lower()
    t = re.sub(r'[\[\(\{].*?[\]\)\}]', '', t)
    t = re.sub(r'\.(zip|rar|cbz|cbr|7z)$', '', t)
    t = re.sub(r'[._\-,|]', ' ', t)
    return ' '.join(t.split())

def resolve_target_folder(base_title):
    """æ™ºèƒ½æ–‡ä»¶å¤¹åŒ¹é…ï¼šå¦‚æœ duplicates ä¸‹å­˜åœ¨ç›¸ä¼¼æ–‡ä»¶å¤¹ï¼Œåˆ™å¤ç”¨"""
    proposed_name = sanitize_filename(base_title)
    if not proposed_name: proposed_name = "Unknown_Folder"
    
    if not os.path.exists(DUPLICATES_ROOT):
        return proposed_name

    existing_dirs = [d for d in os.listdir(DUPLICATES_ROOT) 
                     if os.path.isdir(os.path.join(DUPLICATES_ROOT, d))]
    
    proposed_clean = clean_title_for_comparison(proposed_name)
    best_match = None
    highest_ratio = 0.0

    for existing_dir in existing_dirs:
        if existing_dir == proposed_name: return existing_dir
        existing_clean = clean_title_for_comparison(existing_dir)
        ratio = difflib.SequenceMatcher(None, proposed_clean, existing_clean).ratio()
        if ratio > highest_ratio:
            highest_ratio = ratio
            best_match = existing_dir

    if highest_ratio > 0.90 and best_match:
        return best_match
    
    return proposed_name

def is_safe_duplicate(title_a, title_b, ratio, threshold=0.90):
    if ratio < threshold: return False
    nums_a = re.findall(r'\d+', title_a)
    nums_b = re.findall(r'\d+', title_b)
    if nums_a != nums_b: return False
    return True

def move_file_and_archive(file_path, db, sub_folder_name):
    """
    [ä¿®æ”¹ç‰ˆ] æ‰§è¡Œç‰©ç†ç§»åŠ¨ï¼Œå¹¶æ›´æ–°æ•°æ®åº“è·¯å¾„ (è€Œä¸æ˜¯åˆ é™¤è®°å½•)
    """
    try:
        # 1. æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(file_path):
            print(f"   âš ï¸ æ–‡ä»¶æœªæ‰¾åˆ°: {file_path}")
            # å¦‚æœæ–‡ä»¶æœ¬èº«ä¸å­˜åœ¨ï¼Œå¯ä»¥é€‰æ‹©æ¸…ç†æ­»é“¾ï¼Œæˆ–è€…è·³è¿‡
            # db.archive_and_delete_record(file_path) 
            return False
            
        # 2. å‡†å¤‡ç›®æ ‡è·¯å¾„
        target_dir = os.path.join(DUPLICATES_ROOT, sub_folder_name)
        if not os.path.exists(target_dir):
            os.makedirs(target_dir)
            
        file_name = os.path.basename(file_path)
        name_part, ext_part = os.path.splitext(file_name)
        timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
        
        # 3. é˜²å†²çªå‘½å
        new_name = f"{timestamp}_{file_name}"
        dest_path = os.path.join(target_dir, new_name)
        
        counter = 1
        while os.path.exists(dest_path):
            new_name = f"{timestamp}_{name_part}_{counter}{ext_part}"
            dest_path = os.path.join(target_dir, new_name)
            counter += 1
        
        # 4. ç‰©ç†ç§»åŠ¨æ–‡ä»¶
        real_dest_path = shutil.move(file_path, dest_path)
        
        # 5. [æ ¸å¿ƒä¿®æ”¹] æ›´æ–°æ•°æ®åº“è·¯å¾„
        # ä¸å†åˆ é™¤è®°å½•ï¼Œè€Œæ˜¯å°† file_path æ›´æ–°ä¸º duplicates ä¸‹çš„æ–°è·¯å¾„
        cursor = db.conn.cursor()
        update_sql = f"UPDATE {db.table_name} SET file_path = ? WHERE file_path = ?"
        cursor.execute(update_sql, (real_dest_path, file_path))
        db.conn.commit()
        
        print(f"   ğŸ”„ [å·²ç§»åŠ¨+æ›´æ–°DB] ...{file_name[-15:]} -> {sub_folder_name}")
        return True

    except Exception as e:
        print(f"   âŒ ç§»åŠ¨/æ›´æ–°å¤±è´¥: {e}")
        return False

# ================= é˜¶æ®µ 1: æ•°æ®æ”¶é›† =================

def collect_url_groups(db):
    print("   ğŸ” [1/3] æ‰«æ URL é‡å¤...")
    count = db.find_and_store_url_duplicates() # è¿™ä¸€æ­¥ä¼šå¡«å…… url_duplicates è¡¨
    if count == 0:
        return []
    
    cursor = db.conn.cursor()
    cursor.execute("SELECT * FROM url_duplicates")
    records = cursor.fetchall()
    
    # è½¬æ¢ä¸ºå­—å…¸: {url: [row1, row2...]}
    groups_map = defaultdict(list)
    for row in records:
        groups_map[row['gallery_url']].append(row)
    
    return list(groups_map.values())

def collect_title_groups(db):
    print("   ğŸ” [2/3] æ‰«æ æ ‡é¢˜ ç›¸ä¼¼ (è¿™å¯èƒ½éœ€è¦ä¸€ç‚¹æ—¶é—´)...")
    cursor = db.conn.cursor()
    cursor.execute(f"SELECT * FROM {db.table_name} WHERE status='SUCCESS'")
    all_records = cursor.fetchall()
    
    if not all_records: return []

    # é¢„å¤„ç†
    data_list = []
    for row in all_records:
        raw_title = row['title'] if row['title'] else row['file_name']
        cleaned = clean_title_for_comparison(raw_title)
        data_list.append({
            'row': row,
            'clean_title': cleaned
        })
    
    data_list.sort(key=lambda x: x['clean_title'])
    
    title_groups = []
    total = len(data_list)
    processed_indices = set()
    window_size = 10
    
    for i in range(total):
        if i in processed_indices: continue
        
        item_a = data_list[i]
        current_group = [item_a['row']]
        has_match = False
        
        for j in range(i + 1, min(i + 1 + window_size, total)):
            if j in processed_indices: continue
            
            item_b = data_list[j]
            ratio = difflib.SequenceMatcher(None, item_a['clean_title'], item_b['clean_title']).ratio()
            
            if is_safe_duplicate(item_a['clean_title'], item_b['clean_title'], ratio):
                current_group.append(item_b['row'])
                processed_indices.add(j)
                has_match = True
        
        if has_match:
            processed_indices.add(i)
            title_groups.append(current_group)
            
    return title_groups

# ================= é˜¶æ®µ 2: ç»“æœåˆå¹¶ (æ ¸å¿ƒé€»è¾‘) =================

def merge_and_execute(db):
    print("\nğŸš€ [E-Hentai Scanner] ç»¼åˆå»é‡æ¨¡å¼å¯åŠ¨ (å…¨éƒ¨è½¬ç§»æ¨¡å¼)")
    ensure_duplicates_root()

    # --- 1. è·å–ä¸¤ç»„æ•°æ® ---
    url_groups = collect_url_groups(db)
    print(f"    âœ… URL ç»„æ•°: {len(url_groups)}")
    
    title_groups = collect_title_groups(db)
    print(f"    âœ… æ ‡é¢˜ ç»„æ•°: {len(title_groups)}")

    if not url_groups and not title_groups:
        print("ğŸ‰ æ²¡æœ‰å‘ç°ä»»ä½•é‡å¤æ–‡ä»¶ã€‚")
        return

    print("   ğŸ”— [3/3] æ­£åœ¨åˆå¹¶æ£€æµ‹ç»“æœå¹¶ç”Ÿæˆè¿é€šå›¾...")

    # --- 2. æ„å»ºå›¾ (Adjacency List) ---
    adj = defaultdict(set)
    file_info_map = {} 

    def add_clique_to_graph(group_rows):
        paths = [r['file_path'] for r in group_rows]
        for r in group_rows:
            file_info_map[r['file_path']] = r
            
        for i in range(len(paths)):
            for j in range(i + 1, len(paths)):
                u, v = paths[i], paths[j]
                adj[u].add(v)
                adj[v].add(u)

    for group in url_groups:
        add_clique_to_graph(group)
        
    for group in title_groups:
        add_clique_to_graph(group)

    # --- 3. å¯»æ‰¾è¿é€šåˆ†é‡ (BFS) ---
    visited = set()
    final_clusters = []

    all_nodes = list(file_info_map.keys())
    
    for start_node in all_nodes:
        if start_node in visited:
            continue
            
        cluster = []
        queue = deque([start_node])
        visited.add(start_node)
        
        while queue:
            node = queue.popleft()
            cluster.append(file_info_map[node])
            
            for neighbor in adj[node]:
                if neighbor not in visited:
                    visited.add(neighbor)
                    queue.append(neighbor)
        
        if len(cluster) > 1:
            final_clusters.append(cluster)

    print(f"    âœ… æœ€ç»ˆåˆå¹¶ä¸º {len(final_clusters)} ä¸ªå¤„ç†ç»„ã€‚")
    print("-" * 50)

    # --- 4. æ‰§è¡Œç§»åŠ¨ ---
    total_moved = 0
    
    for idx, cluster in enumerate(final_clusters, 1):
        cluster_with_size = []
        for row in cluster:
            size = 0
            if os.path.exists(row['file_path']):
                size = os.path.getsize(row['file_path'])
            cluster_with_size.append((size, row))
        
        # æŒ‰å¤§å°é™åºæ’åº (ä»…ç”¨äºç¡®å®šæ–‡ä»¶å¤¹å‘½åçš„åŸºå‡†ï¼Œå³æœ€å¤§çš„é‚£ä¸ª)
        cluster_with_size.sort(key=lambda x: x[0], reverse=True)
        
        # 4.1 è·å–ä¿¡æ¯ç”¨äºå‘½å (ä¾ç„¶ä½¿ç”¨æœ€å¤§çš„æ–‡ä»¶æ¥å†³å®šæ–‡ä»¶å¤¹å)
        first_size, first_row = cluster_with_size[0]
        
        # 4.2 ç¡®å®šè¦ç§»åŠ¨çš„æ–‡ä»¶ (æ”¹ä¸ºï¼šå…¨éƒ¨ç§»åŠ¨)
        to_move_list = cluster_with_size
        
        raw_title = first_row['title'] if first_row['title'] else first_row['file_name']
        folder_name = resolve_target_folder(raw_title)
        
        print(f"[{idx}/{len(final_clusters)}] ğŸ“¦ å¤„ç†ç»„ -> {folder_name} (åŒ…å« {len(to_move_list)} ä¸ªæ–‡ä»¶)")
        
        # 4.3 æ‰§è¡Œç§»åŠ¨
        for _, row in to_move_list:
            if move_file_and_archive(row['file_path'], db, folder_name):
                total_moved += 1

    print("-" * 50)
    print(f"ğŸ å…¨éƒ¨å®Œæˆ! å…±ç§»åŠ¨ {total_moved} ä¸ªæ–‡ä»¶ (æ‰€æœ‰é‡å¤ç»„å‡å·²ç§»å…¥ duplicates)ã€‚")

# ================= ä¸»ç¨‹åº =================

def main():
    db = DatabaseManager(config.DB_PATH, table_name=config.TARGET_TABLE)
    try:
        merge_and_execute(db)
    except KeyboardInterrupt:
        print("\nğŸ›‘ ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    finally:
        db.close()

if __name__ == "__main__":
    main()