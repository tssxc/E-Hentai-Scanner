# tools/import_database.py
"""
æ•°æ®åº“å¯¼å…¥/æ¢å¤å·¥å…·
å°† export_database.py ç”Ÿæˆçš„ JSON æ•°æ®å¯¼å…¥åˆ°æ–°æ•°æ®åº“ä¸­
è‡ªåŠ¨é€‚é…å­—æ®µå·®å¼‚ï¼Œæ”¯æŒæ‰¹é‡é«˜é€Ÿå†™å…¥
"""
import sys
import json
import logging
import sqlite3
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
current_dir = Path(__file__).resolve().parent
project_root = current_dir.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

from app.database import DatabaseManager
from app import config

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')
logger = logging.getLogger(__name__)

def import_table_from_json(db, table_name, json_file):
    """ä» JSON å¯¼å…¥æ•°æ®åˆ°æŒ‡å®šè¡¨"""
    if not json_file.exists():
        logger.warning(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡: {json_file.name}")
        return

    logger.info(f"ğŸ“¥ æ­£åœ¨å¯¼å…¥: {table_name} (æº: {json_file.name})")
    
    try:
        with open(json_file, 'r', encoding='utf-8') as f:
            data_list = json.load(f)
            
        if not data_list:
            logger.info("   â„¹ï¸ JSON æ•°æ®ä¸ºç©º")
            return

        # 1. è·å–ç›®æ ‡è¡¨çš„åˆ—ç»“æ„ (é€‚é…æ–°æ•°æ®åº“)
        db.cursor.execute(f"PRAGMA table_info({table_name})")
        columns_info = db.cursor.fetchall()
        # è·å–æ‰€æœ‰åˆ—å
        valid_columns = {col['name'] for col in columns_info}
        
        # 2. å‡†å¤‡ SQL è¯­å¥
        # åŠ¨æ€æ„å»ºåˆ—ååˆ—è¡¨ï¼Œç¡®ä¿åªæ’å…¥æ•°æ®åº“ä¸­å­˜åœ¨çš„åˆ—
        sample_record = data_list[0]
        # æ‰¾å‡º JSON å’Œ æ•°æ®åº“ å…±æœ‰çš„åˆ—
        insert_keys = [k for k in sample_record.keys() if k in valid_columns]
        
        if not insert_keys:
            logger.error("   âŒ æ— æ³•åŒ¹é…ä»»ä½•åˆ—ï¼Œå¯¼å…¥å¤±è´¥")
            return

        columns_str = ", ".join(insert_keys)
        placeholders = ", ".join(["?"] * len(insert_keys))
        
        # ä½¿ç”¨ INSERT OR IGNORE å¿½ç•¥ä¸»é”®å†²çª (ä¿ç•™æ—§ID)
        # æˆ–è€…ä½¿ç”¨ INSERT OR REPLACE (è¦†ç›–æ—§æ•°æ®)
        sql = f"INSERT OR IGNORE INTO {table_name} ({columns_str}) VALUES ({placeholders})"
        
        # 3. è½¬æ¢æ•°æ®ä¸ºå…ƒç»„åˆ—è¡¨ (æ‰¹é‡å¤„ç†)
        batch_data = []
        for item in data_list:
            # æŒ‰é¡ºåºæå–å€¼
            batch_data.append([item.get(k) for k in insert_keys])

        # 4. æ‰§è¡Œæ‰¹é‡æ’å…¥
        db.cursor.executemany(sql, batch_data)
        db.conn.commit()
        
        logger.info(f"   âœ… æˆåŠŸå¯¼å…¥ {db.cursor.rowcount} æ¡è®°å½•")

    except Exception as e:
        db.conn.rollback()
        logger.error(f"   âŒ å¯¼å…¥å¤±è´¥: {e}")

def main():
    logger.info("ğŸš€ å¼€å§‹æ•°æ®åº“æ¢å¤/è¿ç§»...")
    data_dir = config.DATA_DIR
    
    # å¾…å¯¼å…¥çš„è¡¨æ˜ å°„å…³ç³» (JSONæ–‡ä»¶å -> æ•°æ®åº“è¡¨å)
    # ä½ å¯ä»¥æ ¹æ®éœ€è¦æ·»åŠ æ›´å¤šè¡¨
    tasks = [
        ("export_scan_results.json", "scan_results"),
        ("export_duplicates_archive.json", "duplicates_archive"),
        # å…¼å®¹æµ‹è¯•è¡¨
        ("export_scan_results_test.json", "scan_results_test"),
    ]

    try:
        # è¿æ¥æ•°æ®åº“ (ä¼šè‡ªåŠ¨åˆ›å»ºæ–°è¡¨ç»“æ„)
        with DatabaseManager(config.DB_PATH) as db:
            # å¼€å¯æ˜¾å¼äº‹åŠ¡ä»¥æé«˜é€Ÿåº¦
            db.cursor.execute("BEGIN TRANSACTION")
            
            for json_name, table_name in tasks:
                json_path = data_dir / json_name
                # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™è·³è¿‡ (é˜²æ­¢å¯¼å…¥åˆ°æœªåˆå§‹åŒ–çš„è¡¨)
                # DatabaseManager._init_schema åº”è¯¥å·²ç»åˆ›å»ºäº† scan_results
                import_table_from_json(db, table_name, json_path)
            
            db.conn.commit()
            logger.info("ğŸ‰ æ‰€æœ‰å¯¼å…¥ä»»åŠ¡å®Œæˆï¼")
            
    except Exception as e:
        logger.error(f"âŒ ä¸¥é‡é”™è¯¯: {e}")

if __name__ == "__main__":
    main()